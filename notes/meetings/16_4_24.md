# PhD Update 16/4/2024  

## Update

- Canberra last week
  - Worked on/presented Thompson sampling HTPS
  - Thesis outline with Michael 

- Thompson sampling HTPS
  - Need HTPS benchmark, so running that now
  - How to train model to predict prior parameters?
    - Marginal likelihood (couldn't get it to work, larger a/b continues to give the best loss)
    - Other options:
      - Take the posterior after a proof attempt, then predict these parameters
        - Issue with this is that we don't want the parameters too large, as it will take longer for the model to update 
      - Could have the model predict a value in e.g [0,1], take this as a, and then b = 1 - a 
        - Lower value for prior gives less weight to the noisy prediction model
        - Could also factor in the LLM logprob as done in HTPS 
          - Could give a setup where we don't use a predicted prior at all, rather initialise it to the LLM logprob
           
## TODO
- Run and debug HTPS. Need to get it to a point where it beats BestFS (so we 'replicate' the results in the paper)
- Implement and run first pass of Thompson sampling 