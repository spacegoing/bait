# @package _global_
defaults:
  - /base/logging_config@_here_
  - /base/exp_config@_here_
  - env/leandojo
  - tac/lean_reprover
  - search/htps

# set the environment, tactic and search models above


# set experiment name, project, resume status/path and offline/online
exp_config:
  name: htps
  experiment: end_to_end

logging_config:
  project: end_to_end
  offline: false


# whether to shuffle the loaded theorems before evaluation
shuffle: false

# Number of End-to-End Eval -> Train -> Eval loops
num_iterations: 1 #2

# The iteration to resume from
resume_iteration: 0

# Total time allowed for a single proof attempt
total_timeout: 6000

# Maximum time allowed in environment before timing out
env_timeout: 600

log_level: 'INFO'


# Resource configuration


# number of physical GPUs
num_gpus: 1

# how many 'logical' GPUs available, should be a multiple of num_gpus.
# Will control the number of separate tactic/search model processes
logical_gpus: 1

num_cpus: 16

# How much GPU memory to allocate per prover (should be low, since the model is using most of the GPU)
gpu_per_prover: 0.01

# CPU resources to allocate per prover.
cpu_per_prover: 1

# How many provers to assign per logical GPU. Each of these provers will share one tactic/search process.
provers_per_gpu: 2

# Commands to run for retraining the model(s) after every evaluation
train_after_eval: null
#  - python3 -m experiments.lightning_runner --config-name=end_to_end/train/goal_model/run data_module.trace_files=${exp_config.directory}/traces
#  - python3 -m experiments.lightning_runner --config-name=end_to_end/train/gen_seq2seq/run data_module.trace_files=${exp_config.directory}/traces

# The model and its checkpoint attribute to update.
# Must be same length as train_after_eval, with each index corresponding to the associated command
update_checkpoints: null
#    - [ search_model, ckpt_path ]
#  - [tac_model, ckpt_path]
