# @package _global_
defaults:
  - /base/logging_config@_here_
  - /base/exp_config@_here_

exp_config:
  name: train_data_with_fails
  experiment: goal_model

logging_config:
  project: goal_model
  offline: false

model:
  _target_: models.end_to_end.search_models.goal_model.model.SimpleGoalModel
  model_name: kaiyuy/leandojo-lean3-tacgen-byt5-small
  lr: 1e-5
  warmup_steps: 200
  provable_tok: '<extra_id_1>'
  unprovable_tok: '<extra_id_2>'
  max_seq_len: 2300

data_module:
  _target_: models.end_to_end.search_models.goal_model.datamodule.GoalProvableDataModule
  model_name: kaiyuy/leandojo-lean3-tacgen-byt5-small
  batch_size: 1 # effective_batch_size == batch_size * accumulate_grad_batches * devices
  eval_batch_size: 4
  max_seq_len: 2300
  num_workers: 0
  # special tokens for goal scoring
  critic_tok: '<extra_id_0>'
  provable_tok: '<extra_id_1>'
  unprovable_tok: '<extra_id_2>'
  
  trace_files: runs/end_to_end/original/2024_02_15/00_47_29/traces/0 # test traces from 1 iter
  # runs/train_traces/0 # train traces

  replace: 'keep' # add/keep/drop files wrt the existing collection

  # runs/seq2seq/seq2seq_eval_iteration_1/2024_03_01/21_46_35/traces/0


trainer:
  accelerator: gpu
  devices: 1
  precision: bf16-mixed
  strategy:
    _target_: lightning.pytorch.strategies.DeepSpeedStrategy
    stage: 2
    offload_optimizer: false
    cpu_checkpointing: false
    logging_batch_size_per_gpu: 1
  gradient_clip_val: 1.0

  val_check_interval: 5000
  limit_val_batches: 2000

  callbacks:
    - _target_: lightning.pytorch.callbacks.LearningRateMonitor
      logging_interval: step
    - _target_: lightning.pytorch.callbacks.ModelCheckpoint
      verbose: true
      save_top_k: 3
      save_last: true
      monitor: val_acc
      mode: max
      dirpath: ${exp_config.directory}/checkpoints
      auto_insert_metric_name: true
      filename: "{epoch}-{step}-{val_acc:.2f}"
#    - _target_: lightning.pytorch.callbacks.EarlyStopping
#      monitor: val_acc
#      patience: 10
#      mode: max
#      verbose: true
